{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f91c627b-b1d5-4be3-b058-b3f4d6b7d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.insert(1, \"../\")  \n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275dfa8-4b99-41c7-b92b-520b36b3fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/alexander/miniconda3/envs/aiFora360/lib/python3.8/site-packages/aif360/data/raw/compas/compas-scores-two-years.csv')\n",
    "df.info()\n",
    "print(df['sex'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06e408-0a1f-4caf-b6a7-77ecfe0eeb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig = CompasDataset(\n",
    "    protected_attribute_names=['sex'],\n",
    "    privileged_classes=[['Female']],\n",
    "    features_to_drop=['race', 'age']\n",
    ")\n",
    "\n",
    "dataset_orig_train, dataset_orig_val, dataset_orig_test = dataset_orig.split([0.5,0.8], shuffle=True)\n",
    "\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dafd2f1d-c9e1-43a3-be97-79aa933fea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(train=None, val=None, test=None):\n",
    "    if train is not None:\n",
    "        display(Markdown(\"#### Training Dataset shape\"))\n",
    "        print(train.features.shape)\n",
    "    if val is not None:\n",
    "        display(Markdown(\"#### Validation Dataset shape\"))\n",
    "        print(val.features.shape)\n",
    "    display(Markdown(\"#### Test Dataset shape\"))\n",
    "    print(test.features.shape)\n",
    "    display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "    print(test.favorable_label, test.unfavorable_label)\n",
    "    display(Markdown(\"#### Protected attribute names\"))\n",
    "    print(test.protected_attribute_names)\n",
    "    display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "    print(test.privileged_protected_attributes, \n",
    "          test.unprivileged_protected_attributes)\n",
    "#     display(Markdown(\"#### Dataset feature names\"))\n",
    "#     print(test.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb1ea1-16f2-41fe-9ce9-7f1cdeef8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(train=dataset_orig_train, val=dataset_orig_val, test=dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054374ae-fed8-4d07-bf41-ec989abda2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train,\n",
    "                                            privileged_groups=privileged_groups,\n",
    "                                            unprivileged_groups=unprivileged_groups)\n",
    "analyze(metric=metric_orig_train)\n",
    "# TODO: Add explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1593efa1-2d19-4e9b-8176-2a2343c5a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_orig_train\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      LogisticRegression(solver='liblinear', random_state=1))\n",
    "fit_params = {'logisticregression__sample_weight': dataset.instance_weights}\n",
    "\n",
    "lr_orig_panel = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31375b69-d5d1-483d-bcc0-77419786e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def test(dataset, model, thresh_arr):\n",
    "    try:\n",
    "        # sklearn classifier\n",
    "        y_val_pred_prob = model.predict_proba(dataset.features)\n",
    "        pos_ind = np.where(model.classes_ == dataset.favorable_label)[0][0]\n",
    "    except AttributeError:\n",
    "        # aif360 inprocessing algorithm\n",
    "        y_val_pred_prob = model.predict(dataset).scores\n",
    "        pos_ind = 0\n",
    "    \n",
    "    metric_arrs = defaultdict(list)\n",
    "    for thresh in thresh_arr:\n",
    "        y_val_pred = (y_val_pred_prob[:, pos_ind] > thresh).astype(np.float64)\n",
    "\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = y_val_pred\n",
    "        metric = ClassificationMetric(\n",
    "                dataset, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "        metric_arrs['bal_acc'].append((metric.true_positive_rate()\n",
    "                                     + metric.true_negative_rate()) / 2)\n",
    "        metric_arrs['avg_odds_diff'].append(metric.average_odds_difference())\n",
    "        metric_arrs['disp_imp'].append(metric.disparate_impact())\n",
    "        metric_arrs['stat_par_diff'].append(metric.statistical_parity_difference())\n",
    "        metric_arrs['eq_opp_diff'].append(metric.equal_opportunity_difference())\n",
    "        metric_arrs['theil_ind'].append(metric.theil_index())\n",
    "    \n",
    "    return metric_arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a6c1531-83ea-4cc9-adf0-f3e2adc74800",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_val,\n",
    "                   model=lr_orig_panel,\n",
    "                   thresh_arr=thresh_arr)\n",
    "lr_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43fc871e-5d06-4e67-a2f2-c29fc905bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_metrics(metrics, thresh_arr):\n",
    "    best_ind = np.argmax(metrics['bal_acc'])\n",
    "    print(\"Threshold corresponding to Best balanced accuracy: {:6.4f}\".format(thresh_arr[best_ind]))\n",
    "    print(\"Best balanced accuracy: {:6.4f}\".format(metrics['bal_acc'][best_ind]))\n",
    "#     disp_imp_at_best_ind = np.abs(1 - np.array(metrics['disp_imp']))[best_ind]\n",
    "    disp_imp_at_best_ind = 1 - min(metrics['disp_imp'][best_ind], 1/metrics['disp_imp'][best_ind])\n",
    "    print(\"Corresponding 1-min(DI, 1/DI) value: {:6.4f}\".format(disp_imp_at_best_ind))\n",
    "    print(\"Corresponding average odds difference value: {:6.4f}\".format(metrics['avg_odds_diff'][best_ind]))\n",
    "    print(\"Corresponding statistical parity difference value: {:6.4f}\".format(metrics['stat_par_diff'][best_ind]))\n",
    "    print(\"Corresponding equal opportunity difference value: {:6.4f}\".format(metrics['eq_opp_diff'][best_ind]))\n",
    "    print(\"Corresponding Theil index value: {:6.4f}\".format(metrics['theil_ind'][best_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16bb6140-1a8a-4edb-92de-1a7d6e1d6ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.0100\n",
      "Best balanced accuracy: 0.4925\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.4367\n",
      "Corresponding average odds difference value: 0.0057\n",
      "Corresponding statistical parity difference value: 0.0113\n",
      "Corresponding equal opportunity difference value: 0.0164\n",
      "Corresponding Theil index value: 0.7761\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "523c9d1c-8866-4d14-b063-b8f4446fc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_orig_metrics = test(dataset=dataset_orig_test,\n",
    "                       model=lr_orig_panel19,\n",
    "                       thresh_arr=[thresh_arr[lr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3e350bdf-fbcb-486d-8680-239e175ccf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.0100\n",
      "Best balanced accuracy: 0.4946\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.5989\n",
      "Corresponding average odds difference value: 0.0105\n",
      "Corresponding statistical parity difference value: 0.0131\n",
      "Corresponding equal opportunity difference value: 0.0185\n",
      "Corresponding Theil index value: 0.7974\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(lr_orig_metrics, [thresh_arr[lr_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b5e57-fb8f-4d3e-b03d-b66b9cd4059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_orig_train\n",
    "model_rf = make_pipeline(StandardScaler(),\n",
    "                      RandomForestClassifier(n_estimators=500, min_samples_leaf=25))\n",
    "fit_params = {'randomforestclassifier__sample_weight': dataset.instance_weights}\n",
    "rf_orig_panel = model_rf.fit(dataset.features, dataset.labels.ravel(), **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb363f17-6a8f-4585-810f-bc043b0a09d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n",
      "divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "thresh_arr_rf = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics_rf = test(dataset=dataset_orig_val,\n",
    "                   model=rf_orig_panel,\n",
    "                   thresh_arr=thresh_arr_rf)\n",
    "rf_orig_best_ind = np.argmax(val_metrics_rf['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94f9b47d-8c37-4aa2-b0b2-4db39a6ec034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.0100\n",
      "Best balanced accuracy: 0.5000\n",
      "Corresponding 1-min(DI, 1/DI) value:    nan\n",
      "Corresponding average odds difference value: 0.0000\n",
      "Corresponding statistical parity difference value: 0.0000\n",
      "Corresponding equal opportunity difference value: 0.0000\n",
      "Corresponding Theil index value: 0.7848\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(val_metrics_rf, thresh_arr_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "12a43cb9-fda4-4b74-8611-114d6897feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "rf_orig_metrics = test(dataset=dataset_orig_test,\n",
    "                       model=rf_orig_panel,\n",
    "                       thresh_arr=[thresh_arr_rf[rf_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be6e5e-9f0b-44f7-ae95-6ca6ba6c1c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
